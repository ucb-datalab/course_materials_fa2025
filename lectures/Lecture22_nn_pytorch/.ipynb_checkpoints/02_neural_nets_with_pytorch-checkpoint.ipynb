{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning Frameworks\n",
    "\n",
    "AY 128 (UC Berkeley; 2012-2025)\n",
    "\n",
    "\n",
    "Almost all frameworks written in low-level C++/C with Python (or other scripting bindings)\n",
    "\n",
    "### Low-level frameworks\n",
    "\n",
    "   - Tensorflow (Google) Nov 2015. See https://www.tensorflow.org/api_docs/python/tf\n",
    "   - pytorch (Python). https://pytorch.org/docs/stable/index.html\n",
    "   - Theano\n",
    "   - Caffe (Berkeley)\n",
    "   - Torch (Lua)\n",
    "   - CNTK (Microsoft)\n",
    "   - Chainer\n",
    "   - PaddlePaddle (Baidu) Aug 2016\n",
    "   \n",
    "### High-level frameworks (Python)\n",
    "\n",
    "   - Keras (atop Tensorflow, Theano) - https://www.tensorflow.org/api_docs/python/tf/keras\n",
    "   - Skorch - scikit-learn compatible neural network library that wraps PyTorch (https://github.com/skorch-dev/skorch)\n",
    "   - FastAI: https://docs.fast.ai/\n",
    "   - PyTorch Lightning (https://github.com/PyTorchLightning/pytorch-lightning)\n",
    "   \n",
    "<img src=\"figs/frameworks.png\" width=\"75%\">\n",
    "Source: https://paperswithcode.com/trends\n",
    "\n",
    "\n",
    "see also: https://github.com/mbadry1/Top-Deep-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.asimovinstitute.org/wp-content/uploads/2016/09/neuralnetworks.png\">\n",
    "\n",
    "Source: http://www.asimovinstitute.org/neural-network-zoo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example (from Josh's student work): \n",
    "\n",
    "<img src=\"https://github.com/profjsb/deepCR/raw/master/imgs/network.png\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/profjsb/deepCR/master/imgs/postage-sm.jpg\">\n",
    "\n",
    "\"deepCR: Deep Learning Based Cosmic Ray Removal for Astronomical Images\"\n",
    "https://github.com/profjsb/deepCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "   - syntax closely resembles Python, making it easy for developers familiar with the language to transition to deep learning.\n",
    "   - Dynamic Computational Graphs:\n",
    "Unlike some frameworks that require you to define the entire computation graph upfront (static graphs), PyTorch builds the graph on the fly as you execute your code. This makes debugging easier and allows for more flexibility, especially in research settings where rapid prototyping is crucial.\n",
    "   - PyTorch leverages the power of GPUs to accelerate computations, making it suitable for training large and complex models efficiently.                                                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up the California housing data as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "cal_data = datasets.fetch_california_housing()\n",
    "X = cal_data['data']   # 8 features \n",
    "Y = cal_data['target'] # response (median house price)\n",
    "\n",
    "half = math.floor(len(Y)/2)\n",
    "train_X = X[:half]\n",
    "train_Y = Y[:half]\n",
    "test_X = X[half:]\n",
    "test_Y = Y[half:]\n",
    "\n",
    "# scale the data; remove mean and scale to unit variance: z = (x - u) / s\n",
    "# StandardScaler transforms your data such that its distribution will have a mean value 0 and standard deviation of 1\n",
    "scaler = StandardScaler()  \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(train_X)  \n",
    "train_X = scaler.transform(train_X)  \n",
    "\n",
    "# apply same transformation to test data\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_features = train_X.shape[1]\n",
    "print(f'number of input features = {num_input_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Print torch version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a simple neural network in PyTorch, as an MLP (multi-layer perceptron) with\n",
    "a few layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClf(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple neural network classifier using PyTorch.\n",
    "\n",
    "    This class defines a neural network with three hidden layers and one output layer.\n",
    "    It uses ReLU (Rectified Linear Unit) activation function for the hidden layers.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    layer1 : nn.Linear\n",
    "        The first linear layer which takes input features and maps them to 32 neurons.\n",
    "    layer2 : nn.Linear\n",
    "        The second linear layer which takes 32 neurons from the first layer and maps them to another 32 neurons.\n",
    "    layer3 : nn.Linear\n",
    "        The third linear layer which takes 32 neurons from the second layer and maps them to 10 neurons.\n",
    "    output_layer : nn.Linear\n",
    "        The output layer which takes 10 neurons from the third layer and maps them to a single output.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    forward(x):\n",
    "        Defines the forward pass of the neural network. It takes an input tensor `x` and passes it through\n",
    "        the layers with ReLU activation functions in between, and finally through the output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features):\n",
    "        super(NNClf, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_features, 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.layer3 = nn.Linear(32, 10)\n",
    "        self.output_layer = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNClf(num_input_features)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "train_X_tensor = torch.tensor(train_X, dtype=torch.float32)\n",
    "train_Y_tensor = torch.tensor(train_Y, dtype=torch.float32).view(-1, 1)  # reshape to a 2D tensor\n",
    "test_X_tensor = torch.tensor(test_X, dtype=torch.float32)\n",
    "test_Y_tensor = torch.tensor(test_Y, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_Y_tensor.size())\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets & DataLoaders\n",
    "\n",
    "\"Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\"\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorDataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset DataLoader\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "\n",
    "# The criterion is defined as Mean Squared Error (MSE) Loss, which is a common loss function \n",
    "# used for regression tasks. It measures the average squared difference between the \n",
    "# estimated values and the actual value.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# The optimizer is defined as Adam (Adaptive Moment Estimation) optimizer, which is an \n",
    "# algorithm for first-order gradient-based optimization of stochastic objective functions. \n",
    "# It is configured to optimize the parameters of the model with a learning rate of 0.001.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 50  # Number of epochs to train the model\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    # Iterate over batches of data from the training loader\n",
    "    for batch_X, batch_Y in train_loader:\n",
    "        optimizer.zero_grad()  # Clear the gradients of all optimized tensors\n",
    "        outputs = model(batch_X)  # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        loss = criterion(outputs, batch_Y)  # Calculate the loss\n",
    "        loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        optimizer.step()  # Perform a single optimization step (parameter update)\n",
    "    print(f\"{loss.item():0.3f}\", sep=\" \", end=\" \", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()  # set outselves in evaluation mode\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_X_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE\n",
    "mse = np.mean((predictions - test_Y_tensor.numpy()) ** 2)\n",
    "print(\"MSE\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how well did we do?\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"NN Regression Residuals - MSE = %.3f\" % mse)\n",
    "plt.scatter(test_Y_tensor.numpy(), predictions, alpha=0.4, s=3)\n",
    "plt.xlabel(\"Test Y\")\n",
    "plt.ylabel(\"Predicted Y\")\n",
    "plt.plot([0.2, 5], [0.2, 5], c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
