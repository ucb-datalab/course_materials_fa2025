{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNets\n",
    "ResNets are a neural net architecture designed for image classification tasks, but they have a long (for machine learning) history dating back to the 1980s.\n",
    "Generally, they are a solution to the problem that deep neural nets can \"lose track\" of image information in nets that aren't already well-trained. This makes it hard\n",
    "for gradients to be propagated deep into the network to identify useful features. The solution was to introduce \n",
    "skip connections (residual connections, or identity maps) between stacks of convolutional layers to allow gradients to flow directly through the network, mitigating the vanishing gradient problem in very deep networks.\n",
    "The residual blocks allow the model to learn identity mappings easily, making it more robust for deep architectures.\n",
    "\n",
    "<img src=\"ResBlock.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loss Function__: the loss function is a quantitative measure of the distance between two tensors\n",
    "\n",
    "__Learning Rate__: a hyperparameter that controls how much the model's parameters/weights are adjusted during each training epoch\n",
    "\n",
    "__Back propogation__: using gradient information to update the weights at each layer in the network in order to minimize the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradients problem\n",
    "\n",
    "* Consider a network with a learning rate $\\lambda$ and loss function $L$\n",
    "* update the weights such that $w_i^\\prime = w_i + \\Delta w_i = w_i -\\lambda\\frac{\\partial L}{\\partial \\Delta w_i}$\n",
    "    - We're updating each weight by a very small amount based on the loss function and the learning rate\n",
    "    - For example, imagine $\\lambda = 10^{-4}$ and an average gradient of $10^{-15}$, which yields $\\Delta w_i = 10^{-19}$ for each step\n",
    "    - Per the chain rule, this value gets smaller for each back propogation step through the network. Some of the first (and perhaps more informative) layers won't see this change at all\n",
    "\n",
    "### ResNet\n",
    "\n",
    "* A key feature of the ResNet is to introduce __Skip Connections__ that back propograte gradient information by skipping layers of the network.  This was introduced, in part, to help mitigate the vanishing gradients problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Original-ResNet-18-Architecture.png\" width=\"50%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, there are many flavors of residual networks with varying depths and numbers of parameters:\n",
    "\n",
    "<img src=\"resnet_param_counts.png\" width=\"50%\">\n",
    "\n",
    "This general architecture, whereby earlier layers are fed forward with intermediate layers providing augmented context, is also very widely used, including\n",
    "in U-Nets (used for image segmentation), Transformer networks (e.g. the \"T\" in GPT), and many others. But because we are interested in image classification,\n",
    "we'll start with ResNets here.\n",
    "\n",
    "Here's an example of some of the intermediate layers of a ResNet-18 that was trained on a very large and generic set of millions of labeled images, and is now being shown a hops berry (I think?).\n",
    "\n",
    "<img src=\"resnet_feature_visual.png\" width=\"50%\">\n",
    "\n",
    "What's interesting here is how generic the filter shapes are, especially in the first few layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have large training sets, and, in many cases, to augment our training sets with transformations in order to avoid overfitting.\n",
    "Well, it turns out that training a ResNet on a huge number of images that have nothing to do with galaxies can lead it to identify features\n",
    "that are generally useful for image classification, and these features can help jump-start galaxy classification.\n",
    "\n",
    "Even better, someone else spend a huge amount of computing time getting you these weights, so you can leverage trainings that might have taken months on large GPU clusters to run.\n",
    "\n",
    "Harnessing pre-trained networks for new tasks is called **learning transfer**, and it can be quite powerful.\n",
    "The trick is knowning where to create the splice between your pre-trained network and a custom network that focuses on your particular task."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RDMy90B6-PEy",
    "IXWdI2td-PE5"
   ],
   "name": "02_CNNs-pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
